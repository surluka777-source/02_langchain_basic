{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321183c9",
   "metadata": {},
   "source": [
    "이번에는 init_chat_model()를 이용하지 않고(이와같은 함수를 팩토리 함수라고 한다) 다이렉트로 llm 을 불러와 사용 해 보도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  필요한 모듈은 비슷하나 팩토리 함수는 안불러와도 된다\n",
    "# 대신 google 생성형 AI 래퍼를 직접 불러온다 GoogleGenerativeAI \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate     # 랭체인에서 제공하는 대화방식의 프롬프트 템플릿\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI     # 랭체인에서 제공하는 구글 생성형 AI 래퍼\n",
    "from dotenv import load_dotenv                            # .env 파일에서 환경변수를 로드하는 함수\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser # 랭체인에서 제공하는 문자열 출력 파서\n",
    "\n",
    "load_dotenv()                                            # .env 파일에서 환경변수를 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08215e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자 그럼 바로 다이렉트로 GoogleGenerativeAI 클래스를 사용해보자\n",
    "\n",
    "llm_model = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\", temperature=1.0, max_output_tokens=1024)\n",
    "\n",
    "response = llm_model.invoke(\"지구의 자전주기를 설명 해 줄래?\")  # 직접 질문을 넣어서 호출\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 프롬프트 템플릿을 사용 해 보자 파싱까지 해 본다.\n",
    "\n",
    "# llm 모델 선택은 똑같다\n",
    "llm_model = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\", temperature=1.0)\n",
    "\n",
    "# 프롬프트 템플릿을 만든다\n",
    "prompt = ChatPromptTemplate.from_template(\"지구의 자전주기를 설명 해 줄래?\")\n",
    "\n",
    "# 이제 체인 만들어 주면 된다\n",
    "chain2 = prompt | llm_model | StrOutputParser()\n",
    "\n",
    "response2 = chain2.invoke({})    # 프롬프트에 변수가 없으므로 빈 딕셔너리 전달\n",
    "\n",
    "print(response2)                # 결과물 출력\n",
    "\n",
    "# 결과물을 보려면 윗단계의 실행결과를 clear  해 줘야 한다. (주피터스 노트북 특징)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 단순한 from_template 방식이 아닌 메세지 형식의 프롬프트 템플릿을 사용 해 보자\n",
    "\n",
    "# llm 모델 선택은 똑같다\n",
    "llm_model = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\", temperature=1.0)\n",
    "\n",
    "# from_template 이 아니라 from_messages 방식을 사용해서 프롬프트 템플릿을 만든다 \n",
    "# 약간 사용 방법이 틀리다. system, user, assistant 메세지 형식을 지원한다\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 아주 똑똑한 지구과학자야.문어체로 답변한다\"),                                # system 메세지\n",
    "    (\"human\", \"지구에 대해서 설명해줘, 문어체로 해줘\"),                                        # human 메세지\n",
    "    (\"ai\", \"지구는 태양계의 3번째 행성이다. 영어로는 earth 라고 한다.\"),            # ai 메세지 \n",
    "    (\"human\", \"지구의 자전주기를 설명 해 줄래?\")                                  # user 메세지\n",
    "])\n",
    "\n",
    "# 이제 체인 만들어 주면 된다\n",
    "chain3 = prompt | llm_model | StrOutputParser()\n",
    "\n",
    "response3 = chain3.invoke({})    # 프롬프트에 변수가 없으므로 빈 딕셔너리 전달\n",
    "print(response3)                # 결과물 출력\n",
    "\n",
    "# 결과물을 보려면 윗단계의 실행결과를 clear  해 줘야 한다. (주피터스 노트북 특징)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
